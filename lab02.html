<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Evaluation | Farduus Ibraahim</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="lab02.html" class="active">Lab 2: AI Evaluation</a>
            <!-- Add more lab links as the semester progresses -->
        </nav>
        <h1>AI Tool Evaluation</h1>
    </header>

    <main>
        <section>
            <h2>Introduction</h2>
            <p>AI tools are becoming a regular part of how students write, research, and analyze information. Instead of thinking about these tools in abstract terms, this lab pushed me to actually use them and evaluate how they perform in practice. I used a structured framework, Capabilities, Appropriate Use, Ethical Considerations, and Transparency to assess two AI tools I personally tested, ChatGPT and Perplexity. My evaluation is based directly on my experience using each tool, including where they were helpful, where they frustrated me, and where they clearly fell short.</p>
        </section>

        <section>
            <h2>Tool 1: ChatGPT</h2>
            <h3>Capabilities</h3>
            <p>From using ChatGPT, I noticed that it is very strong at organizing ideas and explaining concepts clearly, especially when I gave detailed prompts. When I asked it to explain digital culture, the response was easy to understand and well structured. However, when I followed up by asking for sources, the quality dropped. It either gave very general references or sources that were not clearly connected to specific claims. This showed me that while ChatGPT sounds confident, it does not always provide verifiable or precise information.</p>

            <h3>Appropriate Use</h3>
            <p>ChatGPT worked best as a starting point. It helped me think through ideas and generate options I could refine myself. However, I noticed that if I accepted its responses without questioning them, I would miss inaccuracies or oversimplifications. It would be inappropriate to use ChatGPT for final academic writing or factual claims without checking everything myself. In my workflow, it fits best at the brainstorming and drafting stage, not the final stage.</p>

            <h3>Ethical Considerations</h3>
            <p>While using ChatGPT, I noticed that its explanations often reflect a neutral or dominant perspective and rarely acknowledge marginalized viewpoints unless explicitly prompted. This made me more aware of how bias can appear quietly through omission. There is also an ethical concern around academic integrity, since the tool makes it very easy to generate polished text that could be misused if students are not careful.</p>

            <figure>
                <img src="images/digital-culture.png" alt="ChatGPT explaining digital culture in simplified language" width="600">
                <figcaption>ChatGPT explaining digital culture in simplified language</figcaption>
            </figure>
        
            <h3>Transparency & Verification</h3>
            <p>Because ChatGPT does not consistently provide reliable citations, I had to independently verify information. This reinforced the importance of disclosing AI use and not presenting AI-generated content as my own original analysis.</p>
        
        </section>

        
        
        <section>
            <h2>Tool 2: Perplexity</h2>
            <h3>Capabilities</h3>
            <p>Perplexity immediately felt different from ChatGPT. Its responses were more restrained but clearly sourced. When I clicked the citations, I could see where the information came from, which made me trust it more for research. However, it struggled with open-ended or reflective questions. When I tried to push it toward idea generation, the responses felt rigid and less helpful.</p>

            <h3>Appropriate Use</h3>
            <p>Perplexity worked best for fact-checking and early research. It helped me quickly locate credible sources and summaries. A poor use case would be asking it to generate original arguments or reflective writing, as it lacks flexibility. In my workflow, Perplexity works best after brainstorming, when I need to ground ideas in evidence.
            </p>

            <h3>Ethical Considerations</h3>
            <p>Although Perplexity improves transparency by citing sources, it still relies on existing published material, which may carry its own biases. I also noticed how easy it would be to accept cited information without critically reading the sources, which places responsibility back on the user.</p>

            <figure>
                <img src="images/perplexity.png" alt="Description of screenshot" width="600">
                <figcaption>Perplexity response with cited sources</figcaption>
            </figure>
        
            <h3>Transparency & Verification</h3>
            <p>Perplexity models better transparency than ChatGPT, but it still requires human judgment. I had to evaluate whether the sources were relevant and credible, rather than assuming citations equal accuracy.</p>
    
        </section>

        <section>
            <h2>Broader Reflections</h2>
            <p>Using these tools side by side helped me realize that AI tools are not interchangeable. ChatGPT supports creativity and thinking through ideas, while Perplexity supports verification and research. In the DCDA program, AI tools can enhance learning, but only if students remain active decision-makers. What remains uniquely human is the ability to question, interpret, and ethically evaluate information within cultural and social contexts.

As AI tools continue to evolve, I now know to ask: What does this tool actually help me do? Where does it fail? And how does it shape the way I think?</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2026 Farduus Ibraahim | <a href="https://github.com/farduusibraahim/dcda-portfolio">GitHub</a></p>
    </footer>
</body>
</html>